## 实现方法
使用的是参考优化方法, 即分块方法. 重述一下, 有3个阶段, 分别对应3个kernel函数, 循环的次数是一行划分的块数, 设划分后每行的块数为`num_block`. 不同的阶段写入的graph部分是无交的, 合并起来是整个graph, 后一阶段依赖前一阶段的结果.

### 减少用时的想法
- 减少对graph的访存, 因为graph用的是global memory, 速度最慢, 尽量使用寄存器或者共享内存.
 但是否载入也取决于访存次数是否多, 如果访存次数只有2次内, 则完全没有必要, 因为载入和写回也需要2次, 而且使用更多的共享内存会使得active warps数减少.
- 使用循环展开`#pragma unroll`.

### 三个阶段
- 第一阶段(`blocked_fw_phase1`)对对角块使用Floyd-Warshall 算法. 每次循环只有一个对角块, 因此线程块数为1. 由于循环中会反复读写graph, 为了减少对graph的访存, 创建了大小为一个块大小的共享内存`cache_diag`. 首先, 各线程将本线程负责的`graph[global_row_id][global_col_id]`写入到`cache_diag[idy][idx]`. 同步后进行循环. 由于更新式依赖其它线程的结果, 每次循环开始前或者结束后需要同步. 最后, 将`cache_diag`的结果写回graph.
- 第二阶段(`blocked_fw_phase2`)处理对角块十字交叉的块(除了对角块). 横纵2部分, 每部分有`num_block-1`个块, 不过, 为了写起来方便, grid size还是为`num_block, 2`, 因为在kernel中很容易判断是否是对角块. 这一阶段中, 对于行块, `a[i][j]=a[i][k]+a[k][j]`, 第一部分`a[i][k]`是第一阶段已经计算好的, 第二阶段`a[k][j]`正是本线程块需要更新的. 对于列块, 则相反. 两种情况, 都是有2个块的内存需要频繁访问, 一个是本线程块负责的块, 一个是对角块. 因此需要将两个块的内存载入共享内存中. 同步后执行Floyd-Warshall 算法的循环. 由于更新式中的一部分是依赖本线程块的其它线程的(另一部分则不依赖), 因此每一次循环前或后都需要同步. 最后写回到graph中.
- 第三阶段(`blocked_fw_phase3`)处理剩下的块. grid size是块数\*块数, 即`num_block,num_block`, 由于并不处理前两个阶段处理过的块, 函数一开始就会判断是不是十字块中的块. 更新式`a[i][j]=a[i][k]+a[k][j]`中, `a[i][k]`对应第二阶段列块的内容, `a[k][j]`对应第二阶段行块的内容, 完全不依赖`a[i][j]`本身. 这意味着, 对graph的需求主要就是两个块, 行块和列块, 把它们载入到共享内存中. 至于本线程负责的块, 则不需要载入, 因为只需要在循环结束后写一次就可以了. 循环中也不需要同步, 因为更新表达式完全不依赖本线程块的结果.

## thread block大小的设定
我试过32和16, 结果32比16快得多, 在10000图中, 16的时间是1727, 但32是1177. 猜测原因是因为, 如果块大小是32, 一个thread block访问子矩阵的同一行, 恰好取128字节, 一次transaction即可使同一个warp取完一行.

## 加速比
| 图大小(n) | 朴素实现运行时间(ms) | 实现运行时间(ms) | 加速比  |
| --------- | -------------------- | ---------------- | ------- |
| 1000      | 15.067789            | 2.322646         | 6.48734 |
| 2500      | 377.922046           | 23.203915        | 16.287  |
| 5000      | 2972.494911          | 153.083264       | 19.4175 |
| 7500      | 10015.976964         | 504.164477       | 19.8665 |
| 10000     | 22626.655287         | 1177.495694      | 19.2159 |
